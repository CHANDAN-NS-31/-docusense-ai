# app.py (Premium SaaS-Styled Version)

import os
import time
import textwrap
import requests
import streamlit as st
from dotenv import load_dotenv
from utils import redact_sensitive_info
from fpdf import FPDF
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from utils import redact_sensitive_info


# -------------------- CONFIG --------------------
st.set_page_config(page_title="ğŸ“„ DocuSense AI", layout="wide")
st.markdown("""
    <style>
    body {font-size: 1.1rem;}
    .stMarkdown h1 {color: #FF4B4B; font-size: 2.6rem;}
    .stMarkdown p, .stMarkdown li, .stMarkdown div {font-size: 1.05rem !important;}
    </style>
    """, unsafe_allow_html=True)

# -------------------- HEADER --------------------
st.sidebar.title("ğŸ“¥ Upload PDFs")
uploaded_files = st.sidebar.file_uploader("Upload one or more PDFs", type=["pdf"], accept_multiple_files=True)

# â›¨ Sidebar: Redaction toggle
st.sidebar.markdown("## ğŸ›¡ï¸ Privacy Settings")
st.sidebar.toggle("ğŸ”’ Enable Redaction", value=True, key="enable_redaction")


st.title("ğŸ“„ DocuSense AI â€” Multi-PDF Q&A Assistant")
st.caption("Your smart, private PDF research assistant ğŸ§  â€” powered by LLMs")

# -------------------- ENV + API --------------------
load_dotenv()
API_KEY = os.getenv("OPENROUTER_API_KEY")
MODEL = "meta-llama/llama-3-8b-instruct"

# -------------------- DATA HANDLING --------------------
os.makedirs("pdfs", exist_ok=True)
filenames = []
for file in uploaded_files:
    filepath = os.path.join("pdfs", file.name)
    with open(filepath, "wb") as f:
        f.write(file.read())
    filenames.append(filepath)

retrievers = {}
if filenames:
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    for fname in filenames:
        docs = splitter.split_documents(PyMuPDFLoader(fname).load())
        db = FAISS.from_documents(docs, embeddings)
        retrievers[fname] = db.as_retriever()

# -------------------- SESSION STATE --------------------
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# -------------------- FUNCTION: OpenRouter --------------------
def call_openrouter_api(prompt):
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://chat.openai.com",
        "X-Title": "DocuSense AI"
    }
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": prompt}]
    }
    try:
        response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=payload)
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"âŒ Error: {e}"

# -------------------- FUNCTION: Export to PDF --------------------
def export_to_pdf(history):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    for i, (q, a) in enumerate(history):
        pdf.multi_cell(0, 10, f"Q{i+1}: {q}\nA{i+1}: {a}\n{'-'*60}")
    pdf.output("DocuSense_QA_Export.pdf")
    st.success("âœ… Exported to DocuSense_QA_Export.pdf")

# -------------------- FUNCTION: Ask Question --------------------
def ask_question(query, retriever):
    results = retriever.get_relevant_documents(query)
    context = "\n\n".join([doc.page_content for doc in results[:2]])
    context = redact_sensitive_info(context)

    # Check for user preference in query
    query_lower = query.lower()
    if any(word in query_lower for word in ["only points", "in points", "bullet points"]):
        mode = "points"
    elif any(word in query_lower for word in ["only summary", "summary only", "in summary", "brief answer"]):
        mode = "summary"
    else:
        mode = "both"

    # Prompt template based on detected mode
    if mode == "points":
        prompt = f"""
Use the following PDF content to answer the user's question **in bullet points only**.

ğŸ“˜ Context:
{context}

â“ Question:
{query}

ğŸ“Œ Answer as bullet points:
"""
    elif mode == "summary":
        prompt = f"""
Use the following PDF content to provide a **brief paragraph summary only** (3â€“5 lines).

ğŸ“˜ Context:
{context}

â“ Question:
{query}

ğŸ§¾ Answer as a paragraph:
"""
    else:
        prompt = f"""
Use the following PDF content to answer the user's question in **two formats**:

ğŸ“˜ Context:
{context}

â“ Question:
{query}

ğŸ§¾ 1. Paragraph Summary (3â€“5 lines):

ğŸ“Œ 2. Bullet Points:
"""

    prompt = redact_sensitive_info(prompt)

    with st.spinner("ğŸ¤– Generating answer..."):
        response = call_openrouter_api(prompt)

    return response


# -------------------- FUNCTION: Summarize PDF --------------------
def summarize_pdf(retriever):
    results = retriever.get_relevant_documents("summarize this document")
    context = "\n\n".join([doc.page_content for doc in results[:4]])
    context = redact_sensitive_info(context)
    prompt = f"""Summarize the following PDF content:\n\n{context}"""
    with st.spinner("ğŸ“„ Summarizing document..."):
        return call_openrouter_api(prompt)

# -------------------- MAIN UI --------------------
if retrievers:
    selected = st.selectbox("ğŸ“š Choose a PDF", filenames, format_func=os.path.basename)
    retriever = retrievers[selected]

    col1, col2 = st.columns([3, 1])
    with col1:
        query = st.text_input("â“ Ask a question about the selected PDF:")
        if st.button("Ask") and query:
            answer = ask_question(query, retriever)
            st.session_state.chat_history.append((query, answer))
            st.success("âœ… Answer:")
            for line in textwrap.wrap(answer.strip(), width=100):
                st.markdown(line)
                time.sleep(0.2)



    with col2:
        if st.button("ğŸ“œ Summarize This PDF"):
            summary = summarize_pdf(retriever)
            st.info(summary)

        if st.button("ğŸ“¥ Export Q&A to PDF"):
            export_to_pdf(st.session_state.chat_history)

    # Show chat history
    if st.session_state.chat_history:
        st.markdown("---")
        st.subheader("ğŸ§  Q&A History")
        for i, (q, a) in enumerate(reversed(st.session_state.chat_history)):
            st.markdown(f"**Q{i+1}:** {q}")
            st.markdown(f"_A{i+1}:_ {a}")
else:
    st.info("ğŸ“‚ Upload one or more PDFs to begin.")
